import os
from dotenv import load_dotenv
import requests
import json
load_dotenv()

def openai_question_answer(api_key, question, context=None, model="gpt-4", temperature=0.7, max_tokens=100, stop=None):
    """
    Generates an answer to a given question based on provided context using the OpenAI API.

    Parameters:
    api_key (str): The API key for accessing the OpenAI API.
    question (str): The question to be answered.
    context (str): Optional context to provide additional information for generating the answer.
    model (str): The model to use for text generation (default is "gpt-4").
    temperature (float): Sampling temperature to control the creativity of the model (default is 0.7).
    max_tokens (int): The maximum number of tokens in the generated answer (default is 100).
    stop (str or list): Optional stop sequence to end the generation.

    Returns:
    str: Answer generated by the OpenAI API.
    """
    if context:
        prompt_content = f"Context: {context}\n\nQuestion: {question}"
    else:
        prompt_content = f"Question: {question}"

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt_content}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stop": stop
    }

    response = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        response_json = response.json()
        generated_answer = response_json["choices"][0]["message"]["content"].strip()
        return generated_answer
    else:
        return f"Error {response.status_code}: {response.text}"


def anthropic_question_answer(api_key, question, context=None, model="claude-3-5-sonnet-20240620", temperature=0.7, max_tokens=1024):
    """
    Generates an answer to a given question based on provided context using the Anthropic API.

    Parameters:
    api_key (str): The API key for accessing the Anthropic API.
    question (str): The question to be answered.
    context (str): Optional context to provide additional information for generating the answer.
    model (str): The model to use for text generation (default is "claude-3-5-sonnet-20240620").
    temperature (float): Sampling temperature to control the creativity of the model (default is 0.7).
    max_tokens (int): The maximum number of tokens in the generated answer (default is 1024).

    Returns:
    str: Answer generated by the Anthropic API.
    """
    if context:
        prompt_content = f"Context: {context}\n\nQuestion: {question}"
    else:
        prompt_content = f"Question: {question}"

    url = "https://api.anthropic.com/v1/messages"
    
    headers = {
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json"
    }

    data = {
        "model": model,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "messages": [
            {"role": "user", "content": prompt_content}
        ]
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        response_json = response.json()
        generated_answer = response_json["content"][0]["text"].strip()
        return generated_answer
    else:
        return f"Error {response.status_code}: {response.text}"

def run_mistral(api_key, user_message, model="mistral-medium-latest"):
    url = "https://api.mistral.ai/v1/chat/completions"
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": user_message}
        ],
        "temperature": 0.7,
        "top_p": 1.0,
        "max_tokens": 512,
        "stream": False,
        "safe_prompt": False,
        "random_seed": 1337
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        response_json = response.json()
        return response_json["choices"][0]["message"]["content"].strip()
    else:
        return f"Error {response.status_code}: {response.text}"

def mistral_question_answer(api_key, question, context=None, model="mistral-medium-latest", temperature=0.7, max_tokens=512):
    """
    Generates an answer to a given question based on provided context using the Mistral API.

    Parameters:
    api_key (str): The API key for accessing the Mistral API.
    question (str): The question to be answered.
    context (str): Optional context to provide additional information for generating the answer.
    model (str): The model to use for text generation (default is "mistral-medium-latest").
    temperature (float): Sampling temperature to control the creativity of the model (default is 0.7).
    max_tokens (int): The maximum number of tokens in the generated answer (default is 512).

    Returns:
    str: Answer generated by the Mistral API.
    """
    if context:
        user_message = f"Context: {context}\n\nQuestion: {question}"
    else:
        user_message = f"Question: {question}"
        
    return run_mistral(api_key, user_message, model=model)
